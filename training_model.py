import ember
import numpy as np

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, PReLU
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_score, f1_score, accuracy_score, ConfusionMatrixDisplay

import matplotlib.pyplot as plt

import pickle

def main():
    # Reads vectorized features from saved file location
    X_train, y_train, X_test, y_test = ember.read_vectorized_features("E:/Dissertation/ember2018")

    # Filters out unlabeled samples
    labelled_samples = (y_train != -1)

    # Hold feature data and labels correspondingly
    X_trained = X_train[labelled_samples]
    Y_trained = y_train[labelled_samples]


    # Scale the features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_trained)
    X_test_scaled = scaler.transform(X_test)


    # Copy of the scaler is saved
    with open("scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)

    model = Sequential()
    model.add(Dense(2000, input_dim=2381))
    model.add(PReLU())
    model.add(Dropout(0.5))
    model.add(Dense(1400))
    model.add(PReLU())
    model.add(Dropout(0.3))
    model.add(Dense(1200))
    model.add(PReLU())
    model.add(Dense(1, activation='sigmoid'))


    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])

    # Stops training if value loss does not improve for consecutive epochs, restores to best epoch
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


    model.fit(x=X_scaled, y=Y_trained, epochs=25, batch_size=256, shuffle=True, validation_split = 0.01, callbacks=[early_stop])
    score = model.evaluate(x=X_test_scaled, y=y_test, batch_size=64)
    

    model.save("model.h5")
    print(model.summary())

    # Evaluate the models performance
    evaluate(model, X_test_scaled, y_test)


def evaluate(model, X_test_scaled, y_test):
    y_pred = model.predict(X_test_scaled)
    y_pred_binary = np.round(y_pred)

    # ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)
    print("ROC AUC: ", roc_auc)

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred_binary)
    print("Confusion Matrix:\n", cm)

    # Precision
    precision = precision_score(y_test, y_pred_binary)
    print("Precision: ", precision)

    # F1 score
    f1 = f1_score(y_test, y_pred_binary)
    print("F1 Score: ", f1)

    # Accuracy
    accuracy = accuracy_score(y_test, y_pred_binary)
    print("Accuracy: ", accuracy)


    # Function to find the TPR for a given FPR
    def find_tpr_at_fpr(fpr, tpr, target_fpr):
        idx = np.argmin(np.abs(fpr - target_fpr))
        return tpr[idx]

    # Find TPR at FPRs of 0.1 and 0.01
    tpr_at_fpr_1 = find_tpr_at_fpr(fpr, tpr, 0.1)
    tpr_at_fpr_01 = find_tpr_at_fpr(fpr, tpr, 0.01)

    # Print the TPR values at FPRs of 1 and 0.1
    print("TPR at FPR 1:", tpr_at_fpr_1)
    print("TPR at FPR 0.1:", tpr_at_fpr_01)


    #
    plt.figure()
    plt.plot(fpr, tpr, color='darkblue', lw=2, label='ROC curve AUC = %0.2f' % roc_auc)
    plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True Positive Rate (TPR)')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.show()


    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Benign", "Malicious"])

    fig, ax = plt.subplots()
    cm_display.plot(cmap=plt.cm.Blues, ax=ax, values_format="d")

    ax.set_title("Confusion Matrix")
    ax.set_facecolor("lightgrey")

    plt.show()
    
if __name__ == "__main__":
    main()

